{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Plot of Examples from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time, datetime\n",
    "import numpy as np\n",
    "\n",
    "# ulist = list(); tlist = list()\n",
    "# for line in open(\"./Tweets/R_SwinePork.txt\"):\n",
    "#     u = line.split()[0]\n",
    "#     #print line.strip().split(\"\\t\")[-1]\n",
    "#     t = datetime.strptime(line.strip().split(\"\\t\")[-1], '%Y-%m-%d %H:%M:%S')\n",
    "#     ulist.append(u)\n",
    "#     tlist.append(t)\n",
    "# print ulist[0:2]\n",
    "# print tlist[0:2]\n",
    "\n",
    "f = open(\"./Tweets/R_MentosCokeDeath.txt\")\n",
    "# f = open(\"./Tweets/N_Airliner.txt\")\n",
    "ulist, tslist = list(), list()\n",
    "for line in f.readlines():\n",
    "    # print(line.split(\"\\t\")[-1])\n",
    "    # uid, timestamp = line.split(\"\\t\")[0], int(line.split(\"\\t\")[-1]) # time.mktime(datetime.datetime.strptime(line.split(\"\\t\")[-1].rstrip(), '%Y-%m-%d %H:%M:%S').timetuple()) # ma\n",
    "    uid, timestamp = line.split(\"\\t\")[0], time.mktime(datetime.datetime.strptime(line.split(\"\\t\")[-1].rstrip(), '%Y-%m-%d %H:%M:%S').timetuple()) # kwon\n",
    "    # uid, timestamp = line.split(\"\\t\")[0],  mktime_tz(parsedate_tz(line.split(\"\\t\")[-1].rstrip()))\n",
    "    # time.mktime(datetime.datetime.strptime(line.split(\"\\t\")[-1].rstrip(), '%a %b %d %H:%M:%S %z %Y').timetuple()) # pheme\n",
    "#     print uid, timestamp\n",
    "#     break\n",
    "    ulist.append(uid)\n",
    "    tslist.append(timestamp)\n",
    "tslist, ulist = zip(*sorted(zip(tslist, ulist)))\n",
    "rel_tslist = np.array(tslist) - np.array(tslist[0])\n",
    "f.close()\n",
    "\n",
    "# print ulist # [0:2]\n",
    "# print rel_tslist/3600/24 # [0:2]\n",
    "# print len(ulist)\n",
    "\n",
    "# ----------------------\n",
    "i_cumulative = []\n",
    "cutoff = 1000000\n",
    "lag = 24\n",
    "\n",
    "i_in_period = 0;\n",
    "hour = lag\n",
    "\n",
    "for uname, timestamp in zip(ulist, rel_tslist):\n",
    "    # print uname, timestamp\n",
    "    timestamp = float(timestamp)/3600.0\n",
    "    if timestamp > cutoff and len(i_cumulative) >= 30: \n",
    "        print \"breaking\"\n",
    "        break\n",
    "    if timestamp > hour:\n",
    "        counter = timestamp / lag - len(i_cumulative) \n",
    "        # if counter > 100: counter=100\n",
    "        for c in range(int(counter)):\n",
    "            i_cumulative.append(i_in_period)\n",
    "            hour += lag\n",
    "            i_in_period = 0\n",
    "        # print \"adding\", counter\n",
    "    i_in_period += 1\n",
    "    # if len(i_cumulative) > 1000: break\n",
    "# print i_cumulative, len(i_cumulative)\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (4, 4),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)\n",
    "%matplotlib inline\n",
    "\n",
    "fig1 = plt.figure(figsize=(4, 4))\n",
    "# and the first axes using subplot populated with data \n",
    "ax1 = fig1.add_subplot(111)\n",
    "line1 = plt.bar(np.arange(len(i_cumulative)), i_cumulative, width=10.0, color='indianred')\n",
    "ax1.set_ylim(0, max(i_cumulative)+2)\n",
    "# ax1.yaxis.tick_left()\n",
    "# ax1.legend(fontsize=12)\n",
    "    \n",
    "# now, the second axes that shares the x-axis with the ax1\n",
    "ax2 = fig1.add_subplot(111, sharex=ax1, frameon=False)\n",
    "cdf = np.cumsum(i_cumulative)*1.0/len(ulist)\n",
    "line2 = ax2.bar(range(len(i_cumulative)), cdf, width=1.0, alpha=0.3, color='steelblue', linewidth=1.0)\n",
    "ax2.set_ylim(0, 1+0.05)\n",
    "ax2.yaxis.tick_right()\n",
    "# ax2.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "plt.legend((line1, line2), ('# Tweets', 'Cum. Freq.'), loc='upper left')\n",
    "\n",
    "plt.xlabel('Time in days', fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('newfake_mentoscoke.png', bbox_inches='tight')\n",
    "# plt.savefig('newtrue_airliner.png', bbox_inches='tight')\n",
    "\n",
    "# np.savetxt(\"y_fake\", i_cumulative)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Empirical: Engagements Per User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_labels = np.loadtxt(\"../../datasets/twitter_ma/labels.txt\")\n",
    "# train_labels = np.loadtxt(\"../../datasets/kwon/labels.txt\")\n",
    "def _read_cascades_file(cascades_filename):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    cascades : list(np.array((None, 2)))\n",
    "        list of user_str, timestamp array (one array per cascade)\n",
    "    \"\"\"\n",
    "    f = open(cascades_filename, \"r\")\n",
    "    cascades = []\n",
    "    for line in f.readlines():\n",
    "        u_t = line.strip(\"\\n\").split(\",\")\n",
    "        u = list(map(str, u_t[0::2]))  # string\n",
    "        t = list(map(float, u_t[1::2]))  # float\n",
    "        cascade = np.vstack([u, t]).transpose()\n",
    "        cascades.append(cascade)\n",
    "    f.close()\n",
    "    return cascades\n",
    "# train_cascades = _read_cascades_file(\"../../datasets/kwon/cascades.txt\")\n",
    "train_cascades = _read_cascades_file(\"../../datasets/twitter_ma/cascades.txt\")\n",
    "print(len(train_cascades), len(train_labels))\n",
    "\n",
    "true_cascades = np.array(train_cascades)[train_labels==0]\n",
    "fake_cascades = np.array(train_cascades)[train_labels == 1]\n",
    "print(len(true_cascades), len(fake_cascades))\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "# TRUE AND FAKE - distribution of engagement counts [Top 2K users]\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "u_t = {}\n",
    "for cas in true_cascades:\n",
    "    for u in cas[:,0]:\n",
    "        if u in u_t: u_t[u] += 1\n",
    "        else: u_t[u] = 1\n",
    "sorted_t = np.array(sorted(u_t.items(), key=operator.itemgetter(1), reverse=True), dtype=np.int32)\n",
    "\n",
    "u_f = {}\n",
    "for cas in fake_cascades:\n",
    "    for u in cas[:,0]:\n",
    "        if u in u_f: u_f[u] += 1\n",
    "        else: u_f[u] = 1\n",
    "sorted_f = np.array(sorted(u_f.items(), key=operator.itemgetter(1), reverse=True), dtype=np.int32)\n",
    "\n",
    "# COMMENTED PLOT\n",
    "# print(sorted_t[0:10])\n",
    "# print(np.sum(sorted_f[:, 1] == 1))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "sns.set(color_codes=False)\n",
    "t = sorted_t[1:, 1]  # kwon  = sorted_t[:, 1]\n",
    "f = sorted_f[:, 1]\n",
    "\n",
    "print(t)\n",
    "print(f)\n",
    "\n",
    "t1 = t[t>5]\n",
    "f1 = f[f>5]\n",
    "t2 = t1[t1<1000]  # Kwon t2 = t1\n",
    "f2 = f1[f1<1000]  # kwon f2 = f1\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "sns.distplot(t2, color=\"green\", label=\"True\");\n",
    "\n",
    "plt.legend(loc='upper right', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0, 0.2) # KWON = 0.12\n",
    "# plt.xlim(-100, 800) ADD BACK IN KWON\n",
    "plt.tight_layout()\n",
    "# plt.savefig('kde_dist_tma_true.png')\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "sns.distplot(f2, color=\"red\", label=\"Fake\");\n",
    "plt.legend(loc='upper right', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0, 0.2)  # KWON = 0.12\n",
    "# plt.xlim(-100, 800) ADD BACK IN KWON\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('kde_dist_tma_fake.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kwon Dataset: Extract retweet links from follows links.txt (prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of connected components in cascade by retweet graph created by follows network\n",
    "\n",
    "cascades_file = \"../../datasets/kwon/cascades.txt\"\n",
    "links_file = \"/home/krsharma/mic_kdd_inflmax/kdd_netinference/kwon/links.txt\"\n",
    "\n",
    "import numpy as np\n",
    "all_cas_users = set()\n",
    "cas_users = []\n",
    "f = open(cascades_file, 'r')\n",
    "for line in f.readlines():\n",
    "    # print(line)\n",
    "    cas_users = np.array(line.split(',')[::2], dtype=np.int64)\n",
    "    # print(cas_users)\n",
    "    all_cas_users.update(set(cas_users))\n",
    "    # break\n",
    "print(\"done\")\n",
    "\n",
    "print(len(all_cas_users))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "list_df = []\n",
    "total_edges = 0\n",
    "chunksize = 10 ** 6\n",
    "for i, chunk in enumerate(pd.read_csv(links_file, chunksize=chunksize, header=None, sep=' ')):\n",
    "    # print(chunk)\n",
    "    df = chunk[chunk[0].isin(cas_users) & chunk[1].isin(cas_users)]\n",
    "    print(\"i\", \"len\", i, len(df))\n",
    "    total_edges += len(df)\n",
    "    list_df.append(df)\n",
    "    # break\n",
    "print(\"total=\", total_edges)\n",
    "# df = pd.read_csv(links_file, header = 4)\n",
    "# print(df)\n",
    "# print('ok')\n",
    "\n",
    "edges_df = pd.concat(list_df)\n",
    "print(len(edges_df))\n",
    "\n",
    "print(edges_df.head())\n",
    "\n",
    "edges_df.to_csv('../../datasets/kwon/rel_links.txt', sep=' ', header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Value: Ratio of Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascades_file = \"../../datasets/kwon/cascades.txt\"\n",
    "links_file = \"../../datasets/kwon/rel_links.txt\"\n",
    "labels_file = \"../../datasets/kwon/labels.txt\"\n",
    "\n",
    "import numpy as np\n",
    "all_cas_users = set()\n",
    "list_cas = []\n",
    "f = open(cascades_file, 'r')\n",
    "for i, line in enumerate(f.readlines()):\n",
    "    #print(line)\n",
    "    cas_times = np.array(line.split(',')[1::2], dtype=np.float32)\n",
    "    cas_users = np.array(line.split(',')[::2], dtype=np.int64)\n",
    "    list_cas.append((cas_users, cas_times))\n",
    "    all_cas_users.update(set(cas_users))\n",
    "#     if i == 3:\n",
    "#         break\n",
    "print(\"done\", len(all_cas_users))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(links_file, sep=' ', header=None)\n",
    "\n",
    "print(df.head()) # 0 follows 1\n",
    "print(len(df))\n",
    "# source -> tweets -> A comments (A follows source)\n",
    "# info diffusion from source to A. (p_sA > 0)\n",
    "# directed edge from s to A.\n",
    "\n",
    "# A follows S\n",
    "# Then S->A edge in graph if time(S) < time(A)\n",
    "# A follows B\n",
    "# Then B->A edge in graph if time(B) < time(A)\n",
    "# Between S and B, keep one with later timestep.\n",
    "#     S A B\n",
    "# S     1\n",
    "# A\n",
    "# B     1\n",
    "# each col should have at most 1 one. keep latest timestep\n",
    "\n",
    "# construct retweet graph for each cascade from follow links (time take last one as parent)\n",
    "import networkx as nx \n",
    "\n",
    "def get_graph_cc(links, casu, cast):\n",
    "    # get times\n",
    "    time_dict = {} # uid -> time\n",
    "    for u, t in zip(casu, cast):\n",
    "        time_dict[u] = t\n",
    "    \n",
    "    linked_user_dict_ = {} # uid -> index\n",
    "    list_linked_user = [] # list of indexed uid\n",
    "    next_user = 0\n",
    "    for index, row in links.iterrows():\n",
    "        u, v = row[0], row[1]\n",
    "        if u not in linked_user_dict_:\n",
    "            linked_user_dict_[u] = next_user\n",
    "            list_linked_user.append(u)\n",
    "            next_user += 1\n",
    "        if v not in linked_user_dict_:\n",
    "            linked_user_dict_[v] = next_user\n",
    "            list_linked_user.append(v)\n",
    "            next_user += 1\n",
    "    set_linked_u = set(linked_user_dict_.keys())\n",
    "    isolated_u = set(casu) - set_linked_u\n",
    "    # print(\"isolated\", len(isolated_u), len(casu))\n",
    "    \n",
    "    total_linked_users = len(linked_user_dict_)\n",
    "    mat = np.zeros((total_linked_users, total_linked_users))\n",
    "    for index, row in links.iterrows():\n",
    "        u, v = row[0], row[1]\n",
    "        if time_dict[v] < time_dict[u]:\n",
    "            mat[linked_user_dict_[v], linked_user_dict_[u]] = 1\n",
    "    # print(mat)\n",
    "    \n",
    "    time_col = np.zeros((total_linked_users))\n",
    "    for i, user in enumerate(list_linked_user):\n",
    "        time_col[i] = time_dict[user]    \n",
    "    \n",
    "    for colno in range(len(mat)):\n",
    "        col = mat[:, colno]\n",
    "        if col.sum() > 1:\n",
    "            ind = np.argmax(col)\n",
    "            mat[:, colno] = 0\n",
    "            mat[ind, colno] = 1\n",
    "    num_cc = len(isolated_u)\n",
    "    \n",
    "    G = nx.from_numpy_matrix(mat, create_using=nx.DiGraph())\n",
    "    cc = nx.weakly_connected_components(G)\n",
    "    a = [len(c) for c in sorted(cc, key=len, reverse=True)]\n",
    "    # print(a)\n",
    "    num_cc += len(a)\n",
    "    return num_cc\n",
    "\n",
    "num_cc = []\n",
    "cas_size = []\n",
    "for cas in list_cas:\n",
    "    casu, cast = cas[0], cas[1]\n",
    "    links = df[df[0].isin(casu) & df[1].isin(casu)]\n",
    "    cas_size.append(len(casu))\n",
    "    if len(links) == 0:\n",
    "        num_cc.append(len(casu))\n",
    "    else:\n",
    "        num_cc.append(get_graph_cc(links, casu, cast))\n",
    "print(num_cc)\n",
    "print(cas_size)\n",
    "\n",
    "labels = np.loadtxt(labels_file)\n",
    "fake_ind = np.where(labels==1)[0]\n",
    "true_ind = np.where(labels==0)[0]\n",
    "number_cc = np.array(num_cc, dtype=np.float32)\n",
    "cascade_size = np.array(cas_size, dtype=np.float32)\n",
    "prop_cc = number_cc / cascade_size\n",
    "# print(prop_cc)\n",
    "\n",
    "print(prop_cc[fake_ind])\n",
    "print(\"\\n\")\n",
    "print(prop_cc[true_ind])\n",
    "\n",
    "\n",
    "a = prop_cc[fake_ind]\n",
    "b = prop_cc[true_ind]\n",
    "print(\",\".join(map(str, a)))\n",
    "print(\",\".join(map(str, b)))\n",
    "# The z-score is 1.87577. The p-value is .03005. The result is significant at p < .05.\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "y = np.sort(prop_cc)[:24] # removing multiple 1.0s\n",
    "\n",
    "d = {}\n",
    "for i, yi in enumerate(y):\n",
    "    d[yi] = i\n",
    "    \n",
    "F_fake = np.zeros((len(y)))\n",
    "for item in sorted(prop_cc[fake_ind]):\n",
    "    # print(item)\n",
    "    loc_in_y = d[item]\n",
    "    F_fake[loc_in_y:]+=1\n",
    "F_f = F_fake/len(fake_ind)\n",
    "\n",
    "F_true = np.zeros((len(y)))\n",
    "for item in sorted(prop_cc[true_ind]):\n",
    "    # print(item)\n",
    "    loc_in_y = d[item]\n",
    "    F_true[loc_in_y:]+=1\n",
    "F_t = F_true/len(true_ind)\n",
    "\n",
    "print(F_f)\n",
    "print(F_t)\n",
    "sample_size_fake = len(fake_ind)\n",
    "sample_size_true = len(true_ind)\n",
    "print(sample_size_fake)\n",
    "print(sample_size_true)\n",
    "\n",
    "D_crit = 1.07 * np.sqrt(1.0/sample_size_fake + 1.0/sample_size_true)\n",
    "print(\"crit\", D_crit)\n",
    "\n",
    "max_val = 0\n",
    "for i, yi in enumerate(y):\n",
    "    o = np.abs(F_f[i] - F_t[i])\n",
    "    if o > max_val:\n",
    "        max_val = o\n",
    "print(\"Dn\", max_val)\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "plt.plot(y, F_f, marker=None, color='indianred', label='Fake')\n",
    "plt.fill_between(y, F_f, alpha=0.2, color='yellow', hatch='\\\\')\n",
    "plt.plot(y, F_t, marker=None, color='steelblue', label='True')\n",
    "plt.fill_between(y, F_f, F_t, alpha=0.5, color='yellow', hatch='\\/', label=None)\n",
    "print(F_t - F_f)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('dist_prop_cc.png')\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "mean_prop_fake = np.mean(prop_cc[fake_ind])\n",
    "mean_prop_true = np.mean(prop_cc[true_ind])\n",
    "print(mean_prop_fake)\n",
    "print(mean_prop_true)\n",
    "sample_size_fake = len(fake_ind)\n",
    "sample_size_true = len(true_ind)\n",
    "print(sample_size_fake)\n",
    "print(sample_size_true)\n",
    "std_prop_fake = np.std(prop_cc[fake_ind])\n",
    "std_prop_true = np.std(prop_cc[true_ind])\n",
    "print(std_prop_fake)\n",
    "print(std_prop_true)\n",
    "# s2_prop_fake = std_prop_fake ** 2\n",
    "# s2_prop_true = std_prop_true ** 2\n",
    "s2_prop_fake = np.sum((prop_cc[fake_ind] - mean_prop_fake) ** 2) / (sample_size_fake-1)\n",
    "s2_prop_true = np.sum((prop_cc[true_ind] - mean_prop_true) ** 2)/ (sample_size_true-1)\n",
    "print(s2_prop_fake)\n",
    "print(s2_prop_true)\n",
    "\n",
    "t_statistic = (mean_prop_fake - mean_prop_true) / np.sqrt(s2_prop_fake/sample_size_fake + s2_prop_true/sample_size_true)\n",
    "print(t_statistic)\n",
    "\n",
    "# The p-value is .003951. (one tail)\n",
    "# The result is significant at p < .01.\n",
    "\n",
    "df_num = ((s2_prop_fake/sample_size_fake + s2_prop_true/sample_size_true)) ** 2\n",
    "df_den = (s2_prop_fake/sample_size_fake) ** 2 / (sample_size_fake-1) + (s2_prop_true/sample_size_true) ** 2 / (sample_size_true-1)\n",
    "df = df_num / df_den\n",
    "print(df)\n",
    "\n",
    "#Aspin-Welch Unequal-Variance T-Test Section\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "f = prop_cc[fake_ind]\n",
    "t = prop_cc[true_ind]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "sns.distplot(f)\n",
    "\n",
    "sns.distplot(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Value: Avg Time Delay between Engagements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cascades_file = \"../../datasets/twitter_ma/cascades.txt\"\n",
    "# links_file = \"../../datasets/ma/rel_links.txt\"\n",
    "# labels_file = \"../../datasets/twitter_ma/labels.txt\"\n",
    "\n",
    "cascades_file = \"../../datasets/kwon/cascades.txt\"\n",
    "# # links_file = \"../../datasets/ma/rel_links.txt\"\n",
    "labels_file = \"../../datasets/kwon/labels.txt\"\n",
    "\n",
    "import numpy as np\n",
    "all_cas_users = set()\n",
    "list_cas = []\n",
    "f = open(cascades_file, 'r')\n",
    "for i, line in enumerate(f.readlines()):\n",
    "    #print(line)\n",
    "    cas_times = np.array(line.split(',')[1::2], dtype=np.float32)\n",
    "    cas_users = np.array(line.split(',')[::2], dtype=np.int64)\n",
    "    list_cas.append((cas_users, cas_times))\n",
    "    all_cas_users.update(set(cas_users))\n",
    "#     if i == 3:\n",
    "#         break\n",
    "print(\"done\", len(all_cas_users))\n",
    "\n",
    "mean_gaps = []\n",
    "cas_lens = []\n",
    "for cas in list_cas:\n",
    "    casu, cast = cas[0], cas[1]\n",
    "    gap = cast[1:] - cast[:-1]\n",
    "    mean_gaps.append(np.mean(gap)/3600)\n",
    "    cas_lens.append(len(casu))\n",
    "# print(mean_gaps)\n",
    "# print(cas_lens)\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "labels = np.loadtxt(labels_file)\n",
    "fake_ind = np.where(labels==1)[0]\n",
    "true_ind = np.where(labels==0)[0]\n",
    "prop_cc = np.array(np.log(mean_gaps))\n",
    "# prop_cc = np.array(mean_gaps)\n",
    "# print(prop_cc)\n",
    "\n",
    "# print(prop_cc[fake_ind])\n",
    "# print(\"\\n\")\n",
    "# print(prop_cc[true_ind])\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "mean_prop_fake = np.mean(prop_cc[fake_ind])\n",
    "mean_prop_true = np.mean(prop_cc[true_ind])\n",
    "\n",
    "print(\"max min log fake = \", max(prop_cc[fake_ind]), min(prop_cc[fake_ind]))\n",
    "print(\"max min log true = \", max(prop_cc[true_ind]), min(prop_cc[true_ind]))\n",
    "\n",
    "print(\"mean log fake=\", mean_prop_fake)\n",
    "print(\"mean log true= \", mean_prop_true)\n",
    "sample_size_fake = len(fake_ind)\n",
    "sample_size_true = len(true_ind)\n",
    "print(\"sample size fake=\", sample_size_fake)\n",
    "print(\"sample size true=\", sample_size_true)\n",
    "std_prop_fake = np.std(prop_cc[fake_ind])\n",
    "std_prop_true = np.std(prop_cc[true_ind])\n",
    "# print(std_prop_fake)\n",
    "# print(std_prop_true)\n",
    "s2_prop_fake = std_prop_fake ** 2\n",
    "s2_prop_true = std_prop_true ** 2\n",
    "# s2_prop_fake = np.sum((prop_cc[fake_ind] - mean_prop_fake) ** 2) / (sample_size_fake-1)\n",
    "# s2_prop_true = np.sum((prop_cc[true_ind] - mean_prop_true) ** 2)/ (sample_size_true-1)\n",
    "# print(s2_prop_fake)\n",
    "# print(s2_prop_true)\n",
    "\n",
    "t_statistic = (mean_prop_fake - mean_prop_true) / np.sqrt(s2_prop_fake/sample_size_fake + s2_prop_true/sample_size_true)\n",
    "print(\"t =\", t_statistic)\n",
    "# denom of t-statistic is std error\n",
    "\n",
    "# degrees of freedom\n",
    "df_num = ((s2_prop_fake/sample_size_fake + s2_prop_true/sample_size_true)) ** 2\n",
    "df_den = (s2_prop_fake/sample_size_fake) ** 2 / (sample_size_fake-1) + (s2_prop_true/sample_size_true) ** 2 / (sample_size_true-1)\n",
    "df = df_num / df_den\n",
    "print(df)\n",
    "\n",
    "\n",
    "# LOG NORMAL: KWON/MA: The p-value is < .00001. engagement mean gaps   # .000134.\n",
    "# log mean fake = 3.612041839638598\n",
    "# log mean true = 1.8372581654132927\n",
    "# avg time between engagements is greater in fake news cascades\n",
    "\n",
    "\n",
    "# Not sure: log (m_true/m_fake) = +- t*stderr [confidence interval]: With 95% confidence, mean fake gap is between \n",
    "# X times and Y times of true mean gap. \n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "f = prop_cc[fake_ind]\n",
    "t = prop_cc[true_ind]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "sns.distplot(f, kde='normal')\n",
    "\n",
    "sns.distplot(t, kde=False, norm_hist=False)\n",
    "# plt.savefig('true_cascade_avgengtime.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mic",
   "language": "python",
   "name": "mic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
