{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      website    fact  object  sentiment_score\n",
      "0  1003856029  N_E312  N_E312          -0.6597\n",
      "1  1013430282  N_E312  N_E312          -0.7149\n",
      "2  1014445069  N_E312  N_E312          -0.7149\n",
      "3   101596871  N_E312  N_E312           0.5267\n",
      "4    10228272  N_E312  N_E312          -0.7412\n",
      "Index([u'website', u'fact', u'object', u'sentiment_score'], dtype='object')\n",
      "                       fact       website  sentiment_score\n",
      "0               N_Airfrance  2.313271e+07        -0.313637\n",
      "1                N_Airliner  1.396921e+07         0.129432\n",
      "2                  N_Amanda  2.438650e+07        -0.103014\n",
      "3                 N_AnnieLe  4.059135e+07        -0.194677\n",
      "4  N_BarnesNobleObamaMonkey  1.760870e+07         0.238274\n",
      "Index([u'fact', u'website', u'sentiment_score'], dtype='object')\n",
      "992\n",
      "992\n",
      "   accuracy  f1-score  precision    recall  support\n",
      "0  0.469758  0.532028   0.474603  0.605263      494\n",
      "1  0.469758  0.388372   0.461326  0.335341      498\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def _metrics_report_to_df(ytrue, ypred):\n",
    "    precision, recall, fscore, support = metrics.precision_recall_fscore_support(ytrue, ypred)\n",
    "    acc = metrics.accuracy_score(ytrue, ypred)\n",
    "    classification_report = pd.concat(map(pd.DataFrame, [[acc,acc], fscore, precision, recall, support]), axis=1)\n",
    "    classification_report.columns = [\"accuracy\", \"f1-score\", \"precision\", \"recall\", \"support\"]\n",
    "    return(classification_report)\n",
    "\n",
    "import pandas as pd\n",
    "# data=\"/meladyfs/newyork/krsharma/kdd_data/kwon/sentiments.txt\"\n",
    "data=\"/meladyfs/newyork/krsharma/kdd_data/twitter-ma/twitter-ma.csv\"\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "print df.head()\n",
    "print df.columns\n",
    "# sys.exit()\n",
    "\n",
    "senti = df.groupby(['fact'], as_index=False).mean()\n",
    "label = (senti['sentiment_score']>0)*1\n",
    "\n",
    "print senti.head()\n",
    "print senti.columns\n",
    "print len(label)\n",
    "\n",
    "ground = []\n",
    "for row in senti['fact']:\n",
    "    if row.split(\"_\")[0] == 'N':\n",
    "        ground.append(0)\n",
    "    else:\n",
    "        ground.append(1)\n",
    "print len(ground)\n",
    "        \n",
    "report = _metrics_report_to_df(ground, label) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "pprint(report)       \n",
    "\n",
    "# Kwon\n",
    "\n",
    "# accuracy  f1-score  precision    recall  support\n",
    "# 0  0.531532  0.527273   0.491525  0.568627       51\n",
    "# 1  0.531532  0.535714   0.576923  0.500000       60\n",
    "\n",
    "# MA\n",
    "# accuracy  f1-score  precision    recall  support\n",
    "# 0  0.486218  0.572477   0.474886  0.720554      433\n",
    "# 1  0.486218  0.356354   0.516000  0.272152      474\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 1.]\n",
      "   accuracy  f1-score  precision    recall  support\n",
      "0  0.365927  0.365288   0.364185  0.366397      494\n",
      "1  0.365927  0.366566   0.367677  0.365462      498\n"
     ]
    }
   ],
   "source": [
    "assigned = np.array(np.loadtxt(\"untitled1.txt\"))\n",
    "print assigned[0:5]\n",
    "ground = np.loadtxt(\"/meladyfs/newyork/krsharma/kdd_data/twitter-ma/train_labels.txt\")\n",
    "print ground[0:5]\n",
    "report = _metrics_report_to_df(ground, assigned) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "pprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 117824 192350\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Number of posts in the cascade\n",
    "# Time: Total time length of cascade, Avg time gap between posts in cascade\n",
    "## ignore tree: Avg Depth of tree, Avg Breadth of tree \n",
    " \n",
    "# data=\"/meladyfs/newyork/krsharma/kdd_data/twitter-ma/train_cascades.txt\"\n",
    "# ground_data=\"/meladyfs/newyork/krsharma/kdd_data/twitter-ma/train_labels.txt\"\n",
    "data=\"/home/krsharma/kdd_netinference/kwon/train_cascades.txt\"\n",
    "ground_data=\"/home/krsharma/kdd_netinference/kwon/train_labels.txt\"\n",
    "import operator, sys\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def extract_feats(ulist, tlist, top_users_set):\n",
    "    total_time = tlist[-1]\n",
    "    num_posts = len(ulist)\n",
    "    time_gap = tlist[1:] - tlist[0:-1]\n",
    "    frac_top_users = len(set(ulist) & top_users_set)*1.0/len(ulist)\n",
    "    return [num_posts, total_time, np.mean(time_gap), frac_top_users]\n",
    "    \n",
    "    # feats = extract_feats(np.array([1,2,3]), np.array([2,5,6]))\n",
    "\n",
    "\n",
    "cascades = []\n",
    "upart = {}\n",
    "\n",
    "tot_eng = 0\n",
    "for line in open(data):\n",
    "    activations = line.split(\" \")\n",
    "    tot_eng += len(activations)\n",
    "    ulist, tlist = [], []\n",
    "    for a in activations:\n",
    "        u, t = a.strip().split(\"-\")\n",
    "        if u not in upart:\n",
    "            upart[u] = 1\n",
    "        else: upart[u] += 1\n",
    "        ulist.append(u); tlist.append(float(t))\n",
    "    cascades.append((ulist, tlist))\n",
    "    # break\n",
    "print \"user\", len(upart), tot_eng\n",
    "sys.exit()\n",
    "    \n",
    "sorted_x = sorted(upart.items(), key=operator.itemgetter(1), reverse=True)\n",
    "top_users = sorted_x[:5000]\n",
    "top_users_set = set()\n",
    "for u, count in top_users:\n",
    "    top_users_set.add(u)\n",
    "\n",
    "X = []\n",
    "for ulist, tlist in cascades:\n",
    "    cas_features = extract_feats(np.array(ulist), np.array(tlist), top_users_set)\n",
    "    X.append(cas_features)\n",
    "\n",
    "X = np.array(X)\n",
    "print X.shape\n",
    "score = np.mean(X, axis=0)\n",
    "score[1] = score[1]/3600.0; score[2] = score[2]/3600.0 \n",
    "print list(score) # [num_posts, total_time, np.mean(time_gap), frac_top_users]\n",
    "# sys.exit()\n",
    "# print X\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# kmeans = KMeans(n_clusters=2, random_state=None).fit(preprocessing.normalize(X))\n",
    "kmeans = KMeans(n_clusters=2, random_state=None).fit(X)\n",
    "assigned = kmeans.labels_# array([0, 0, 0, 1, 1, 1], dtype=int32)\n",
    "# print assigned\n",
    "print len(assigned)\n",
    "\n",
    "ground = np.loadtxt(ground_data)\n",
    "report = _metrics_report_to_df(ground, assigned) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "pprint(report)\n",
    "report = _metrics_report_to_df(ground, 1-assigned) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "pprint(report)\n",
    "# print kmeans.predict([[0, 0], [4, 4]]) # array([0, 1], dtype=int32)\n",
    "# print kmeans.cluster_centers_ # array([[1., 2.], [4., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-123-2f71aaa713e4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-123-2f71aaa713e4>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    accuracy  f1-score  precision    recall  support\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "kwon (unnormalized)\n",
    "accuracy  f1-score  precision    recall  support\n",
    "0  0.405405  0.297872   0.325581  0.274510       51\n",
    "1  0.405405  0.484375   0.455882  0.516667       60\n",
    "\n",
    "   accuracy  f1-score  precision    recall  support\n",
    "0  0.594595  0.621849   0.544118  0.725490       51\n",
    "1  0.594595  0.563107   0.674419  0.483333       60\n",
    "\n",
    "t-ma (normalized)\n",
    "(992, 4)\n",
    "[597.1683467741935, 1982.9470718525986, 64.61494570103345, 0.16557653119819682]\n",
    "992\n",
    "   accuracy  f1-score  precision    recall  support\n",
    "0  0.490927  0.595677   0.492715  0.753036      494\n",
    "1  0.490927  0.312925   0.485232  0.230924      498\n",
    "  accuracy  f1-score  precision    recall  support\n",
    "0  0.509073  0.333789   0.514768  0.246964      494\n",
    "1  0.509073  0.611333   0.507285  0.769076      498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.179 5.111433184538363\n",
      "0.589\n"
     ]
    }
   ],
   "source": [
    "s= [13.36, 0.187, 1.49, 0.589, 0.269]\n",
    "    \n",
    "print np.mean(s), np.std(s)\n",
    "print np.median(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
