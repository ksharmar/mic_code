{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StanceEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         website         fact       object  sentiment_score\n",
      "213856  10433782  N_Airfrance  N_Airfrance          -0.2960\n",
      "213857  10563792  N_Airfrance  N_Airfrance           0.0000\n",
      "213858  11877492  N_Airfrance  N_Airfrance          -0.2960\n",
      "213859  12408202  N_Airfrance  N_Airfrance          -0.5423\n",
      "213860  14437816  N_Airfrance  N_Airfrance          -0.3182\n",
      "213861  14483018  N_Airfrance  N_Airfrance          -0.3182\n",
      "213862  14857813  N_Airfrance  N_Airfrance          -0.2960\n",
      "213863  16154218  N_Airfrance  N_Airfrance          -0.2960\n",
      "213864  16369839  N_Airfrance  N_Airfrance          -0.6908\n",
      "213865  16591984  N_Airfrance  N_Airfrance          -0.2960\n",
      "Index(['website', 'fact', 'object', 'sentiment_score'], dtype='object')\n",
      "                       fact       website  sentiment_score  label\n",
      "0               N_Airfrance  2.313271e+07        -0.313637      1\n",
      "1                N_Airliner  1.396921e+07         0.129432      0\n",
      "2                  N_Amanda  2.438650e+07        -0.103014      1\n",
      "3                 N_AnnieLe  4.059135e+07        -0.194677      1\n",
      "4  N_BarnesNobleObamaMonkey  1.760870e+07         0.238274      0\n",
      "5             N_BeefProtest  1.464837e+07        -0.338311      1\n",
      "6            N_ByrdBillings  3.157210e+07        -0.147243      1\n",
      "7        N_CharlieWilsonWar  1.396259e+07        -0.379277      1\n",
      "8        N_ChristianTheLion  2.239164e+07         0.140572      0\n",
      "9        N_ClarkRockefeller  2.304830e+07        -0.318787      1\n",
      "      fact       website  sentiment_score  label\n",
      "982  R_E87  7.284459e+08        -0.125376      1\n",
      "983  R_E89  1.118533e+09        -0.021176      0\n",
      "984  R_E90  1.036226e+09         0.034532      0\n",
      "985  R_E92  1.454013e+09         0.151967      0\n",
      "986  R_E93  1.331471e+09        -0.274957      1\n",
      "987  R_E94  6.909024e+08        -0.140805      1\n",
      "988  R_E95  1.344207e+09         0.044818      0\n",
      "989  R_E96  7.628278e+08        -0.345963      1\n",
      "990  R_E97  9.017552e+08        -0.609528      1\n",
      "991  R_E99  7.481818e+08        -0.461193      1\n",
      "Index(['fact', 'website', 'sentiment_score', 'label'], dtype='object')\n",
      "992\n",
      "992\n",
      "   accuracy  f1-score  precision    recall  support\n",
      "0  0.534274  0.534274   0.532129  0.536437      494\n",
      "1  0.534274  0.534274   0.536437  0.532129      498\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def _metrics_report_to_df(ytrue, ypred):\n",
    "    precision, recall, fscore, support = metrics.precision_recall_fscore_support(ytrue, ypred)\n",
    "    acc = metrics.accuracy_score(ytrue, ypred)\n",
    "    classification_report = pd.concat(map(pd.DataFrame, [[acc,acc], fscore, precision, recall, support]), axis=1)\n",
    "    classification_report.columns = [\"accuracy\", \"f1-score\", \"precision\", \"recall\", \"support\"]\n",
    "    return(classification_report)\n",
    "\n",
    "import pandas as pd\n",
    "# data=\"/meladyfs/newyork/krsharma/kdd_data/kwon/sentiments.txt\"\n",
    "data=\"/meladyfs/newyork/krsharma/kdd_data/twitter-ma/twitter-ma.csv\"\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "print(df[df['fact'] == 'N_Airfrance'].head(10))\n",
    "print(df.columns)\n",
    "# sys.exit()\n",
    "\n",
    "senti = df.groupby(['fact'], as_index=False).mean()\n",
    "label = (senti['sentiment_score']<=-0.05)*1\n",
    "senti['label'] = label\n",
    "print(senti.head(10))\n",
    "print(senti.tail(10))\n",
    "print(senti.columns)\n",
    "print(len(label))\n",
    "\n",
    "ground = []\n",
    "for row in senti['fact']:\n",
    "    if row.split(\"_\")[0] == 'N':\n",
    "        ground.append(0)\n",
    "    else:\n",
    "        ground.append(1)\n",
    "print(len(ground))\n",
    "        \n",
    "report = _metrics_report_to_df(ground, label) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "pprint(report)   \n",
    "\n",
    "# Kwon\n",
    "\n",
    "# accuracy  f1-score  precision    recall  support\n",
    "# 0  0.531532  0.527273   0.491525  0.568627       51\n",
    "# 1  0.531532  0.535714   0.576923  0.500000       60\n",
    "\n",
    "# MA\n",
    "# accuracy  f1-score  precision    recall  support\n",
    "# 0  0.486218  0.572477   0.474886  0.720554      433\n",
    "# 1  0.486218  0.356354   0.516000  0.272152      474\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 233719 592391.0\n",
      "(992, 4)\n",
      "[597.1683467741935, 1982.9470718525986, 64.61494570103345, 0.16573996807935304]\n",
      "992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.25      0.33       494\n",
      "         1.0       0.51      0.77      0.61       498\n",
      "\n",
      "    accuracy                           0.51       992\n",
      "   macro avg       0.51      0.51      0.47       992\n",
      "weighted avg       0.51      0.51      0.47       992\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.75      0.60       494\n",
      "         1.0       0.49      0.23      0.31       498\n",
      "\n",
      "    accuracy                           0.49       992\n",
      "   macro avg       0.49      0.49      0.45       992\n",
      "weighted avg       0.49      0.49      0.45       992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of posts in the cascade\n",
    "# Time: Total time length of cascade, Avg time gap between posts in cascade\n",
    "## ignore tree: Avg Depth of tree, Avg Breadth of tree \n",
    " \n",
    "# data=\"/meladyfs/newyork/krsharma/kdd_data/twitter-ma/train_cascades.txt\"\n",
    "# ground_data=\"/meladyfs/newyork/krsharma/kdd_data/twitter-ma/train_labels.txt\"\n",
    "# data=\"/home/krsharma/kdd_netinference/kwon/train_cascades.txt\"\n",
    "# ground_data=\"/home/krsharma/kdd_netinference/kwon/train_labels.txt\"\n",
    "\n",
    "# data=\"../data/kwon/cascades.txt\"\n",
    "# ground_data=\"../data/kwon/labels.txt\"\n",
    "\n",
    "data=\"../data/tma/cascades.txt\"\n",
    "ground_data=\"../data/tma/labels.txt\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import operator, sys\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def extract_feats(ulist, tlist, top_users_set):\n",
    "    total_time = tlist[-1]\n",
    "    num_posts = len(ulist)\n",
    "    time_gap = tlist[1:] - tlist[0:-1]\n",
    "    frac_top_users = len(set(ulist) & top_users_set)*1.0/len(ulist)\n",
    "    return [num_posts, total_time, np.mean(time_gap), frac_top_users]\n",
    "    \n",
    "    # feats = extract_feats(np.array([1,2,3]), np.array([2,5,6]))\n",
    "\n",
    "\n",
    "cascades = []\n",
    "upart = {}\n",
    "\n",
    "tot_eng = 0\n",
    "for line in open(data):\n",
    "    # print(line)\n",
    "    activations = line.strip().split(\",\")\n",
    "    tot_eng += len(activations)/2\n",
    "    ulist, tlist = [], []\n",
    "    \n",
    "    users = activations[0::2]\n",
    "    times = activations[1::2]\n",
    "    \n",
    "    for u, t in zip(users, times):\n",
    "        u = int(u)\n",
    "        t = float(t)\n",
    "        if u not in upart:\n",
    "            upart[u] = 1\n",
    "        else: upart[u] += 1\n",
    "        ulist.append(u); tlist.append(float(t))    \n",
    "    cascades.append((ulist, tlist))\n",
    "    # break\n",
    "print(\"user\", len(upart), tot_eng)\n",
    "# sys.exit()\n",
    "    \n",
    "sorted_x = sorted(upart.items(), key=operator.itemgetter(1), reverse=True)\n",
    "top_users = sorted_x[:5000]\n",
    "top_users_set = set()\n",
    "for u, count in top_users:\n",
    "    top_users_set.add(u)\n",
    "\n",
    "X = []\n",
    "for ulist, tlist in cascades:\n",
    "    cas_features = extract_feats(np.array(ulist), np.array(tlist), top_users_set)\n",
    "    X.append(cas_features)\n",
    "\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "score = np.mean(X, axis=0)\n",
    "score[1] = score[1]/3600.0; score[2] = score[2]/3600.0 \n",
    "print(list(score)) # [num_posts, total_time, np.mean(time_gap), frac_top_users]\n",
    "# sys.exit()\n",
    "# print X\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=None).fit(preprocessing.normalize(X))\n",
    "# kmeans = KMeans(n_clusters=2, random_state=None).fit(X)\n",
    "assigned = kmeans.labels_# array([0, 0, 0, 1, 1, 1], dtype=int32)\n",
    "# print assigned\n",
    "print(len(assigned))\n",
    "\n",
    "ground = np.loadtxt(ground_data)\n",
    "report = metrics.classification_report(ground, assigned) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "print(report)\n",
    "report = metrics.classification_report(ground, 1-assigned) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "print(report)\n",
    "# print kmeans.predict([[0, 0], [4, 4]]) # array([0, 1], dtype=int32)\n",
    "# print kmeans.cluster_centers_ # array([[1., 2.], [4., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.75      0.60       494\n",
      "         1.0       0.49      0.23      0.31       498\n",
      "\n",
      "    accuracy                           0.49       992\n",
      "   macro avg       0.49      0.49      0.45       992\n",
      "weighted avg       0.49      0.49      0.45       992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(ground, 1 - assigned)\n",
    "print(report)\n",
    "np.savetxt('../output/baselines/tma/kmeans_pred_results.txt', 1 - assigned, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-123-2f71aaa713e4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-123-2f71aaa713e4>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    accuracy  f1-score  precision    recall  support\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "kwon (unnormalized)\n",
    "accuracy  f1-score  precision    recall  support\n",
    "0  0.405405  0.297872   0.325581  0.274510       51\n",
    "1  0.405405  0.484375   0.455882  0.516667       60\n",
    "\n",
    "   accuracy  f1-score  precision    recall  support\n",
    "0  0.594595  0.621849   0.544118  0.725490       51\n",
    "1  0.594595  0.563107   0.674419  0.483333       60\n",
    "\n",
    "t-ma (normalized)\n",
    "(992, 4)\n",
    "[597.1683467741935, 1982.9470718525986, 64.61494570103345, 0.16557653119819682]\n",
    "992\n",
    "   accuracy  f1-score  precision    recall  support\n",
    "0  0.490927  0.595677   0.492715  0.753036      494\n",
    "1  0.490927  0.312925   0.485232  0.230924      498\n",
    "  accuracy  f1-score  precision    recall  support\n",
    "0  0.509073  0.333789   0.514768  0.246964      494\n",
    "1  0.509073  0.611333   0.507285  0.769076      498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF predictions fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Vince\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "   \n",
    "\n",
    "datadir = 'kwon'\n",
    "labels = '../data/kwon/labels.txt'\n",
    "cascade_names = '../data/kwon/cascade_names.txt'\n",
    "save_file = '../output/baselines/kwon/tf_pred_results.txt'\n",
    "\n",
    "# datadir = 'tma'\n",
    "# labels = '../data/tma/labels.txt'\n",
    "# cascade_names = '../data/tma/cascade_names.txt'\n",
    "# save_file = '../output/baselines/tma/tf_pred_results.txt'\n",
    "\n",
    "dict_preds = {}\n",
    "\n",
    "if datadir == 'kwon':\n",
    "    r = pkl.load(open('../clustering_baselines/tf_kwon_results.pkl', 'rb'))\n",
    "    scores = r['dataframe']['trustworthiness']\n",
    "    events = r['dataframe']['fact']\n",
    "    for event, score in zip(events, scores):\n",
    "        # event, score = line.split(',')[0].strip(), float(line.split(',')[1].strip().strip('[]'))\n",
    "        dict_preds[event] = (score < 0)*1\n",
    "        # break\n",
    "elif datadir == 'tma':\n",
    "    f = open('../output/baselines/tma/tf_predictions.txt', 'r')\n",
    "    for line in f.readlines():\n",
    "        if line == '' or line == '\\n':\n",
    "            continue\n",
    "        # print(line)\n",
    "        event, score = line.split(',')[0].strip(), float(line.split(',')[1].strip().strip('[]'))\n",
    "        # print(event, score)\n",
    "        dict_preds[event] = (score < 0)*1\n",
    "        # break\n",
    "    # print(scores)\n",
    "print(list(dict_preds.keys())[0])\n",
    "dict_preds['N_Vince']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.82      0.61        51\n",
      "         1.0       0.62      0.25      0.36        60\n",
      "\n",
      "    accuracy                           0.51       111\n",
      "   macro avg       0.55      0.54      0.48       111\n",
      "weighted avg       0.56      0.51      0.47       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "cnames = np.loadtxt(cascade_names, dtype=np.str)\n",
    "gt = np.loadtxt(labels)\n",
    "# print(cnames)\n",
    "# # print(gt)\n",
    "\n",
    "tar = []\n",
    "preds = []\n",
    "count0 = 0\n",
    "for i, name in enumerate(cnames):\n",
    "    n = re.sub('.txt$', '', name)\n",
    "    if n in dict_preds:\n",
    "        p = dict_preds[n]\n",
    "        preds.append(p)\n",
    "    else:\n",
    "        preds.append(0)\n",
    "        count0 += 1\n",
    "\n",
    "print(count0)\n",
    "print(metrics.classification_report(gt, preds))\n",
    "\n",
    "np.savetxt(save_file, preds, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mic",
   "language": "python",
   "name": "mic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
